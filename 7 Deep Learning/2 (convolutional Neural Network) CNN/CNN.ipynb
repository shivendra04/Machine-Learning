{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a specialized type of neural network model designed for working with two-dimensional image data, although they can be used with one-dimensional and three-dimensional data.\n",
    "\n",
    "a convolution is a linear operation that involves the multiplication of a set of weights with the input, much like a traditional neural network. Given that the technique was designed for two-dimensional input, the multiplication is performed between an array of input data and a two-dimensional array of weights, called a filter or a kernel.\n",
    "\n",
    "The filter is smaller than the input data and the type of multiplication applied between a filter-sized patch of the input and the filter is a dot product.\n",
    "\n",
    "Using a filter smaller than the input is intentional as it allows the same filter (set of weights) to be multiplied by the input array multiple times at different points on the input. Specifically, the filter is applied systematically to each overlapping part or filter-sized patch of the input data, left to right, top to bottom.\n",
    "\n",
    "This systematic application of the same filter across an image is a powerful idea. If the filter is designed to detect a specific type of feature in the input, then the application of that filter systematically across the entire input image allows the filter an opportunity to discover that feature anywhere in the image. This capability is commonly referred to as translation invariance, e.g. the general interest in whether the feature is present rather than where it was present.\n",
    "\n",
    "The output from multiplying the filter with the input array one time is a single value. As the filter is applied multiple times to the input array, the result is a two-dimensional array of output values that represent a filtering of the input. As such, the two-dimensional output array from this operation is called a “feature map“\n",
    "\n",
    "Convolutional neural networks do not learn a single filter; they, in fact, learn multiple features in parallel for a given input.\n",
    "\n",
    "For example, it is common for a convolutional layer to learn from 32 to 512 filters in parallel for a given input.\n",
    "\n",
    "A filter must always have the same number of channels as the input, often referred to as “depth“. If an input image has 3 channels (e.g. a depth of 3), then a filter applied to that image must also have 3 channels (e.g. a depth of 3). In this case, a 3×3 filter would in fact be 3x3x3 or [3, 3, 3] for rows, columns, and depth. Regardless of the depth of the input and depth of the filter, the filter is applied to the input using a dot product operation which results in a single value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-1 Data preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'C:\\\\Users\\\\BizAct-110\\\\OneDrive\\\\MachineLearning\\\\Section 40 - Convolutional Neural Networks (CNN)\\\\dataset\\\\training_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "#shear = shift one part of an image, a layer, a selection or a path to a direction and the other part to the opposite direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        'C:\\\\Users\\\\BizAct-110\\\\OneDrive\\\\MachineLearning\\\\Section 40 - Convolutional Neural Networks (CNN)\\\\dataset\\\\test_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn=tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step-1 Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu',input_shape=[64,64,3]))\n",
    "#rectified linear unit()relu\n",
    "#TF and Keras expects image dimension as (Width, Height, Channels), channels being 3 for RGB images and 1 for greyscale images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step-2 pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2\n",
    "#Role of pooling layer is to reduce the resolution of the feature map but retaining features of the map required for \n",
    "#classification through translational and rotational invariants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding 2nd convolutin layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step-3 flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step-4 full connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128,activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step-5 Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-3 Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the CNN on training data set and evaluating it on test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 250 steps, validate for 63 steps\n",
      "Epoch 1/25\n",
      "250/250 [==============================] - 176s 703ms/step - loss: 0.6692 - accuracy: 0.5755 - val_loss: 0.6251 - val_accuracy: 0.6475\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 82s 329ms/step - loss: 0.5860 - accuracy: 0.6880 - val_loss: 0.5516 - val_accuracy: 0.7185\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 81s 324ms/step - loss: 0.5383 - accuracy: 0.7308 - val_loss: 0.5373 - val_accuracy: 0.7420\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 87s 347ms/step - loss: 0.5145 - accuracy: 0.7415 - val_loss: 0.4901 - val_accuracy: 0.7675\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 85s 341ms/step - loss: 0.5046 - accuracy: 0.7551 - val_loss: 0.4821 - val_accuracy: 0.7775\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 71s 285ms/step - loss: 0.4659 - accuracy: 0.7805 - val_loss: 0.5049 - val_accuracy: 0.7620\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 69s 278ms/step - loss: 0.4604 - accuracy: 0.7825 - val_loss: 0.5007 - val_accuracy: 0.7665\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 64s 255ms/step - loss: 0.4429 - accuracy: 0.7900 - val_loss: 0.5066 - val_accuracy: 0.7710\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 63s 251ms/step - loss: 0.4338 - accuracy: 0.7922 - val_loss: 0.4732 - val_accuracy: 0.7725\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 62s 249ms/step - loss: 0.4240 - accuracy: 0.8000 - val_loss: 0.4863 - val_accuracy: 0.7735\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 62s 248ms/step - loss: 0.4062 - accuracy: 0.8099 - val_loss: 0.4525 - val_accuracy: 0.8030\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 62s 248ms/step - loss: 0.3955 - accuracy: 0.8188 - val_loss: 0.4635 - val_accuracy: 0.7860\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 62s 247ms/step - loss: 0.3821 - accuracy: 0.8284 - val_loss: 0.4519 - val_accuracy: 0.7935\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 62s 247ms/step - loss: 0.3761 - accuracy: 0.8269 - val_loss: 0.4548 - val_accuracy: 0.8035\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 62s 247ms/step - loss: 0.3544 - accuracy: 0.8415 - val_loss: 0.4573 - val_accuracy: 0.7925\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 62s 248ms/step - loss: 0.3405 - accuracy: 0.8471 - val_loss: 0.4586 - val_accuracy: 0.7960\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 62s 248ms/step - loss: 0.3341 - accuracy: 0.8524 - val_loss: 0.4591 - val_accuracy: 0.7975\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 62s 248ms/step - loss: 0.3192 - accuracy: 0.8619 - val_loss: 0.4492 - val_accuracy: 0.8045\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 62s 248ms/step - loss: 0.3051 - accuracy: 0.8655 - val_loss: 0.5381 - val_accuracy: 0.7920\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 63s 250ms/step - loss: 0.2926 - accuracy: 0.8715 - val_loss: 0.4527 - val_accuracy: 0.8110\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 62s 249ms/step - loss: 0.2773 - accuracy: 0.8823 - val_loss: 0.4593 - val_accuracy: 0.8010\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 0.2662 - accuracy: 0.8870 - val_loss: 0.5539 - val_accuracy: 0.7895\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 63s 250ms/step - loss: 0.2709 - accuracy: 0.8830 - val_loss: 0.5118 - val_accuracy: 0.7890\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 62s 250ms/step - loss: 0.2509 - accuracy: 0.8945 - val_loss: 0.5061 - val_accuracy: 0.7990\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 62s 250ms/step - loss: 0.2362 - accuracy: 0.9010 - val_loss: 0.5309 - val_accuracy: 0.7960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2e0ac192208>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=training_set,validation_data=test_set,epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-4 Making a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image=image.load_img('C:\\\\Users\\\\BizAct-110\\\\OneDrive\\\\MachineLearning\\\\Section 40 - Convolutional Neural Networks (CNN)\\\\dataset\\\\single_prediction\\\\cat_or_dog_1.jpg',target_size=[64,64])\n",
    "test_image=image.img_to_array(test_image)\n",
    "test_image=np.expand_dims(test_image,axis=0)\n",
    "result=cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0]==1:\n",
    "  prediction='dog'\n",
    "else:\n",
    "  prediction='cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
